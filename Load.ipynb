{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles retrieved from document_parses: 68905\n"
     ]
    }
   ],
   "source": [
    "current = os.getcwd() #current directory\n",
    "data_folder = '/CORD-19-research-challenge/'\n",
    "\n",
    "#The data folders\n",
    "folders = ['document_parses']\n",
    "\n",
    "#Counting the files\n",
    "paths = dict()\n",
    "for folder in folders:\n",
    "    f = current + data_folder + folder + '/pdf_json/'\n",
    "    filenames = os.listdir(f)\n",
    "    paths[folder] = filenames\n",
    "\n",
    "for key in paths.keys():\n",
    "    print(\"Number of articles retrieved from {}:\".format(key), len(paths[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from the paths into a list of json text data as dict\n",
    "def load(folder:str) -> list:\n",
    "    all_files = []\n",
    "    for filename in paths[folder]:\n",
    "        file = os.path.join(current + data_folder + folder + '/pdf_json/' + filename)\n",
    "        f = json.load(open(file,'rb'))\n",
    "        all_files.append(f)\n",
    "    return all_files\n",
    "\n",
    "#loaded = ['biorxiv_medrxiv','comm_use_subset','custom_license','noncomm_use_subset','arxiv']\n",
    "\n",
    "def format_body(body_text):\n",
    "    texts = [(di['section'], di['text']) for di in body_text]\n",
    "    texts_di = {di['section']: \"\" for di in body_text}\n",
    "    \n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "\n",
    "    for section, text in texts_di.items():\n",
    "        body += section\n",
    "        body += \"\\n\\n\"\n",
    "        body += text\n",
    "        body += \"\\n\\n\"\n",
    "    \n",
    "    return body\n",
    "\n",
    "def format_name(author):\n",
    "    middle_name = \" \".join(author['middle'])\n",
    "    \n",
    "    if author['middle']:\n",
    "        return \" \".join([author['first'], middle_name, author['last']])\n",
    "    else:\n",
    "        return \" \".join([author['first'], author['last']])\n",
    "\n",
    "\n",
    "def format_authors(authors):\n",
    "    name_ls = []\n",
    "    \n",
    "    for author in authors:\n",
    "        name = format_name(author)\n",
    "        \n",
    "        name_ls.append(name)\n",
    "    \n",
    "    return \", \".join(name_ls)\n",
    "\n",
    "#Take the json, format all the data we need and place into dataframe\n",
    "def finish(data):\n",
    "    clean = []\n",
    "    for f in data:\n",
    "        features = [\n",
    "            f['paper_id'],\n",
    "            f['metadata']['title'],\n",
    "            format_authors(f['metadata']['authors']),\n",
    "            format_body(f['abstract']),\n",
    "            format_body(f['body_text']),\n",
    "        ]\n",
    "        clean.append(features)\n",
    "\n",
    "    column_names = [\n",
    "        'paper_id',\n",
    "        'title',\n",
    "        'authors',\n",
    "        'abstract',\n",
    "        'body_text'\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(clean,columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded documents\n"
     ]
    }
   ],
   "source": [
    "# bio = load('biorxiv_medrxiv')\n",
    "# print('loaded bio')\n",
    "# comm_use_subset = load('comm_use_subset')\n",
    "# print('loaded comm_use_subset')\n",
    "\n",
    "# custom_license = load('custom_license')\n",
    "# print('loaded custom_license')\n",
    "\n",
    "# noncomm_use_subset = load('noncomm_use_subset')\n",
    "# print('loaded noncomm_use_subset')\n",
    "\n",
    "# arxiv = load('arxiv')\n",
    "# print('loaded arxiv')\n",
    "\n",
    "art = load('document_parses')\n",
    "print('loaded documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-cfa0bec1c9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Create combined dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finshed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-7e50d11f90df>\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mformat_authors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mformat_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mformat_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         ]\n\u001b[1;32m     59\u001b[0m         \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-7e50d11f90df>\u001b[0m in \u001b[0;36mformat_body\u001b[0;34m(body_text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtexts_di\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# b = finish(bio)\n",
    "# print('finshed bio')\n",
    "\n",
    "# comm = finish(comm_use_subset)\n",
    "# print('finshed comm_use_subset')\n",
    "\n",
    "# custom = finish(custom_license)\n",
    "# print('finshed custom_license')\n",
    "\n",
    "# n = finish(noncomm_use_subset)\n",
    "# print('finshed noncomm_use_subset')\n",
    "\n",
    "# a = finish(arxiv)\n",
    "# print('finished arxiv')\n",
    "\n",
    "#Create combined dataset\n",
    "big = finish(art)\n",
    "print(\"finshed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lemmatized body text column in data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['fig','virus','il-','ifn','virus','cell','cells','data','model','medrxiv','data','disease','licence',\n",
    "                    'certified','version','author','nc','nd','cc','rsv','reuse','peer','review','international','by',\n",
    "                    'preprint','respondent','http','https','copyright','patient','introduction']\n",
    "\n",
    "def stop_word_add(custom,words:set):\n",
    "    for word in custom:\n",
    "        words.add(word)\n",
    "    return words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word for word in doc.ents]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', str(token)) and len(str(token)) > 1 and str(token) not in stop_words:\n",
    "            filtered_tokens.append(str(token))       \n",
    "    stems = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    l = ', '.join(stems)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_word_add(custom_stop_words,stop_words)\n",
    "\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "big['body_text_new'] = big.apply(lambda row: tokenize_and_lemmatize(row['body_text'].lower()), axis=1)\n",
    "big['abstract_new'] = big.apply(lambda row: tokenize_and_lemmatize(row['abstract'].lower()), axis=1)\n",
    "\n",
    "\n",
    "big['title'] = big['title'].str.lower()\n",
    "\n",
    "#Export to csv file\n",
    "print('saving to csv')\n",
    "big.to_csv('data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEmodel = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# train = USEmodel(data.body_text)\n",
    "# train_m = tf.train.Checkpoint(v=tf.Variable(train))\n",
    "\n",
    "# train_m.f = tf.function( lambda  x: exported_m.v * x, input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "\n",
    "# model = train_m.v.numpy()\n",
    "\n",
    "# pickle.dump(model, open('full_tfidf_model.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# count_data = count_vectorizer.fit_transform(data['body_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SearchDocument(query,size=100):\n",
    "#     q =[query]\n",
    "#     Q_Train = USEmodel(q)\n",
    "    \n",
    "#     linear_similarities = linear_kernel(Q_Train, model).flatten() \n",
    "    \n",
    "#     Top_index_doc = linear_similarities.argsort()[:-(size+1):-1]\n",
    "#     #print(Top_index_doc)\n",
    "#     linear_similarities.sort()\n",
    "#     find = pd.DataFrame()\n",
    "#     for i,index in enumerate(Top_index_doc):\n",
    "#         find.loc[i,'index'] = str(index)\n",
    "#         find.loc[i,'Paper_ID'] = data['paper_id'][index] \n",
    "#         find.loc[i,'Title'] = data['title'][index] \n",
    "#         find.loc[i,'abstract'] = data['abstract'][index]\n",
    "#     for j,simScore in enumerate(linear_similarities[:-(size+1):-1]):\n",
    "#         find.loc[j,'Score'] = simScore\n",
    "        \n",
    "#     if size==1:\n",
    "#         if find.isnull().values.any():\n",
    "#             print(\"Query: \",query,\".  Title of the Research Paper is missing, Paper ID is:\",find.loc[0,'Paper_ID'],\"\\n\")\n",
    "#         else:\n",
    "#             print(\"Query: \",query,\".  Title of the Research Paper:\",find.loc[0,'Title'],\"\\n\")\n",
    "#     else:\n",
    "#         return find"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
