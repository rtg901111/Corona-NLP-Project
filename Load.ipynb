{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All this file does it takes the json files and exports the csv file. Not really much to see here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = os.getcwd() #current directory\n",
    "data_folder = '/CORD-19-research-challenge/'\n",
    "\n",
    "#The data folders\n",
    "folders = ['document_parses']\n",
    "\n",
    "#Counting the files\n",
    "paths = dict()\n",
    "for folder in folders:\n",
    "    f = current + data_folder + folder + '/pdf_json/'\n",
    "    filenames = os.listdir(f)\n",
    "    paths[folder] = filenames\n",
    "\n",
    "for key in paths.keys():\n",
    "    print(\"Number of articles retrieved from {}:\".format(key), len(paths[key]))\n",
    "\n",
    "#loading the data from the paths into a list of json text data as dict\n",
    "def load(folder:str) -> list:\n",
    "    all_files = []\n",
    "    for filename in paths[folder]:\n",
    "        file = os.path.join(current + data_folder + folder + '/pdf_json/' + filename)\n",
    "        f = json.load(open(file,'rb'))\n",
    "        all_files.append(f)\n",
    "    return all_files\n",
    "\n",
    "#loaded = ['biorxiv_medrxiv','comm_use_subset','custom_license','noncomm_use_subset','arxiv']\n",
    "\n",
    "def format_body(body_text):\n",
    "    texts = [(di['section'], di['text']) for di in body_text]\n",
    "    texts_di = {di['section']: \"\" for di in body_text}\n",
    "    \n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "\n",
    "    for section, text in texts_di.items():\n",
    "        body += section\n",
    "        body += \"\\n\\n\"\n",
    "        body += text\n",
    "        body += \"\\n\\n\"\n",
    "    \n",
    "    return body\n",
    "\n",
    "def format_name(author):\n",
    "    middle_name = \" \".join(author['middle'])\n",
    "    \n",
    "    if author['middle']:\n",
    "        return \" \".join([author['first'], middle_name, author['last']])\n",
    "    else:\n",
    "        return \" \".join([author['first'], author['last']])\n",
    "\n",
    "\n",
    "def format_authors(authors):\n",
    "    name_ls = []\n",
    "    \n",
    "    for author in authors:\n",
    "        name = format_name(author)\n",
    "        \n",
    "        name_ls.append(name)\n",
    "    \n",
    "    return \", \".join(name_ls)\n",
    "\n",
    "#Take the json, format all the data we need and place into dataframe\n",
    "def finish(data):\n",
    "    clean = []\n",
    "    for f in data:\n",
    "        features = [\n",
    "            f['paper_id'],\n",
    "            f['metadata']['title'],\n",
    "            format_authors(f['metadata']['authors']),\n",
    "            format_body(f['abstract']),\n",
    "            format_body(f['body_text']),\n",
    "        ]\n",
    "        clean.append(features)\n",
    "\n",
    "    column_names = [\n",
    "        'paper_id',\n",
    "        'title',\n",
    "        'authors',\n",
    "        'abstract',\n",
    "        'body_text'\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(clean,columns=column_names)\n",
    "    return df\n",
    "\n",
    "# bio = load('biorxiv_medrxiv')\n",
    "# print('loaded bio')\n",
    "# comm_use_subset = load('comm_use_subset')\n",
    "# print('loaded comm_use_subset')\n",
    "\n",
    "# custom_license = load('custom_license')\n",
    "# print('loaded custom_license')\n",
    "\n",
    "# noncomm_use_subset = load('noncomm_use_subset')\n",
    "# print('loaded noncomm_use_subset')\n",
    "\n",
    "# arxiv = load('arxiv')\n",
    "# print('loaded arxiv')\n",
    "\n",
    "art = load('document_parses')\n",
    "print('loaded documents')\n",
    "\n",
    "# b = finish(bio)\n",
    "# print('finshed bio')\n",
    "\n",
    "# comm = finish(comm_use_subset)\n",
    "# print('finshed comm_use_subset')\n",
    "\n",
    "# custom = finish(custom_license)\n",
    "# print('finshed custom_license')\n",
    "\n",
    "# n = finish(noncomm_use_subset)\n",
    "# print('finshed noncomm_use_subset')\n",
    "\n",
    "# a = finish(arxiv)\n",
    "# print('finished arxiv')\n",
    "\n",
    "#Create combined dataset\n",
    "big = finish(art)\n",
    "print(\"finshed\")\n",
    "\n",
    "big.to_csv('big.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
