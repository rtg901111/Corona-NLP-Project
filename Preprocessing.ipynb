{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "import pickle\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big = pd.read('big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lemmatized body text column in data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['fig','virus','il-','ifn','virus','cell','cells','data','model','medrxiv','data','disease','licence',\n",
    "                    'certified','version','author','nc','nd','cc','rsv','reuse','peer','review','international','by',\n",
    "                    'preprint','respondent','http','https','copyright','patient','introduction']\n",
    "\n",
    "def stop_word_add(custom,words:set):\n",
    "    for word in custom:\n",
    "        words.add(word)\n",
    "    return words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word for word in doc.ents]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', str(token)) and len(str(token)) > 1 and str(token) not in stop_words:\n",
    "            filtered_tokens.append(str(token))       \n",
    "    stems = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    l = ', '.join(stems)\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_word_add(custom_stop_words,stop_words)\n",
    "\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "big['body_text_new'] = big.apply(lambda row: tokenize_and_lemmatize(row['body_text'].lower()), axis=1)\n",
    "big['abstract_new'] = big.apply(lambda row: tokenize_and_lemmatize(row['abstract'].lower()), axis=1)\n",
    "\n",
    "\n",
    "big['title'] = big['title'].str.lower()\n",
    "\n",
    "#Export to csv file\n",
    "print('saving to csv')\n",
    "big.to_csv('data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEmodel = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# train = USEmodel(data.body_text)\n",
    "# train_m = tf.train.Checkpoint(v=tf.Variable(train))\n",
    "\n",
    "# train_m.f = tf.function( lambda  x: exported_m.v * x, input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "\n",
    "# model = train_m.v.numpy()\n",
    "\n",
    "# pickle.dump(model, open('full_tfidf_model.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# count_data = count_vectorizer.fit_transform(data['body_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SearchDocument(query,size=100):\n",
    "#     q =[query]\n",
    "#     Q_Train = USEmodel(q)\n",
    "    \n",
    "#     linear_similarities = linear_kernel(Q_Train, model).flatten() \n",
    "    \n",
    "#     Top_index_doc = linear_similarities.argsort()[:-(size+1):-1]\n",
    "#     #print(Top_index_doc)\n",
    "#     linear_similarities.sort()\n",
    "#     find = pd.DataFrame()\n",
    "#     for i,index in enumerate(Top_index_doc):\n",
    "#         find.loc[i,'index'] = str(index)\n",
    "#         find.loc[i,'Paper_ID'] = data['paper_id'][index] \n",
    "#         find.loc[i,'Title'] = data['title'][index] \n",
    "#         find.loc[i,'abstract'] = data['abstract'][index]\n",
    "#     for j,simScore in enumerate(linear_similarities[:-(size+1):-1]):\n",
    "#         find.loc[j,'Score'] = simScore\n",
    "        \n",
    "#     if size==1:\n",
    "#         if find.isnull().values.any():\n",
    "#             print(\"Query: \",query,\".  Title of the Research Paper is missing, Paper ID is:\",find.loc[0,'Paper_ID'],\"\\n\")\n",
    "#         else:\n",
    "#             print(\"Query: \",query,\".  Title of the Research Paper:\",find.loc[0,'Title'],\"\\n\")\n",
    "#     else:\n",
    "#         return find"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
